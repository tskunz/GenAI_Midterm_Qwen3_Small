{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd1b65a8-4301-444a-bd7c-a6f2bd1df9df",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dd1b65a8-4301-444a-bd7c-a6f2bd1df9df",
    "outputId": "4f762354-e0a3-4cc2-e5d4-e61a227a202c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface_hub version: 0.34.4\n",
      "tokenizers version: 0.22.0\n",
      "torch version: 2.8.0+cpu\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\n",
    "    \"huggingface_hub\",  # to download pretrained weights\n",
    "    \"tokenizers\",       # to implement the tokenizer\n",
    "    \"torch\",            # to implement the model\n",
    "]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e96fbb-8e16-4f6d-835f-c6159321280b",
   "metadata": {},
   "source": [
    "- This notebook supports both the base model and the reasoning (\"thinking\") model; which model to use can be controlled via the following flag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70a90338-624a-4706-aa55-6b4358070194",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_REASONING_MODEL = True\n",
    "# Uses the base model if USE_REASONING_MODEL = False\n",
    "\n",
    "USE_INSTRUCT_MODEL = False\n",
    "# Uses the instruct mode (without reasoning) if \n",
    "# USE_REASONING_MODEL = True\n",
    "# USE_INSTRUCT_MODEL = True\n",
    "# This setting does have no effect if USE_REASONING_MODEL = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653410a6-dd2b-4eb2-a722-23d9782e726d",
   "metadata": {
    "id": "653410a6-dd2b-4eb2-a722-23d9782e726d"
   },
   "source": [
    "&nbsp;\n",
    "# 1. Architecture code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82076c21-9331-4dcd-b017-42b046cf1a60",
   "metadata": {
    "id": "82076c21-9331-4dcd-b017-42b046cf1a60"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        # cfg is a dictionary with keys: emb_dim, hidden_dim, dtype, for each of the fc layers we pass it through embedding and hidden dims\n",
    "        # dtype is the data type for the layers, e.g. torch.float32\n",
    "        # fc1 is the projection layer, fc2 is the gating layer, fc3 is the output layer that returns to the original embedding dimension\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(cfg[\"emb_dim\"], cfg[\"hidden_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
    "        self.fc2 = nn.Linear(cfg[\"emb_dim\"], cfg[\"hidden_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
    "        self.fc3 = nn.Linear(cfg[\"hidden_dim\"], cfg[\"emb_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is the input tensor of shape (batch_size, seq_len, emb_dim)\n",
    "        # we apply the three linear layers with a SiLU activation and gating mechanism\n",
    "        # return the output tensor of shape (batch_size, seq_len, emb_dim)\n",
    "        x_fc1 = self.fc1(x)\n",
    "        x_fc2 = self.fc2(x)\n",
    "        x = nn.functional.silu(x_fc1) * x_fc2\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56715760-37e1-433e-89da-04864c139a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    # Root Mean Square Layer Normalization\n",
    "    def __init__(self, emb_dim, eps=1e-6, bias=False, qwen3_compatible=True):\n",
    "        # set up the parameters for the RSMNorm, including the embedding dimension, epsilon for numerical stability\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.qwen3_compatible = qwen3_compatible\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim)) if bias else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # this is where the normalization happens\n",
    "        # x is the input tensor of shape (batch_size, seq_len, emb_dim)\n",
    "        # we compute the variance across the last dimension (emb_dim) and normalize\n",
    "        # we then scale and shift the normalized tensor\n",
    "        # return the normalized tensor of the same shape as input\n",
    "        input_dtype = x.dtype\n",
    "\n",
    "        if self.qwen3_compatible:\n",
    "            x = x.to(torch.float32)\n",
    "\n",
    "        variance = x.pow(2).mean(dim=-1, keepdim=True)\n",
    "        norm_x = x * torch.rsqrt(variance + self.eps)\n",
    "        norm_x = norm_x * self.scale\n",
    "\n",
    "        if self.shift is not None:\n",
    "            norm_x = norm_x + self.shift\n",
    "\n",
    "        return norm_x.to(input_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b9a346f-5826-4083-9162-abd56afc03f0",
   "metadata": {
    "id": "4b9a346f-5826-4083-9162-abd56afc03f0"
   },
   "outputs": [],
   "source": [
    "# rope is rotary positional embedding that is used in the attention mechanism to encode positional information\n",
    "# we precompute the sine and cosine values for the positions and head dimensions\n",
    "# then apply the rotary transformation to the input tensor\n",
    "def compute_rope_params(head_dim, theta_base=10_000, context_length=4096, dtype=torch.float32):\n",
    "    assert head_dim % 2 == 0, \"Embedding dimension must be even\"\n",
    "\n",
    "    # Compute the inverse frequencies\n",
    "    inv_freq = 1.0 / (theta_base ** (torch.arange(0, head_dim, 2, dtype=dtype)[: (head_dim // 2)].float() / head_dim))\n",
    "\n",
    "    # Generate position indices\n",
    "    positions = torch.arange(context_length, dtype=dtype)\n",
    "\n",
    "    # Compute the angles\n",
    "    angles = positions.unsqueeze(1) * inv_freq.unsqueeze(0)  # Shape: (context_length, head_dim // 2)\n",
    "\n",
    "    # Expand angles to match the head_dim\n",
    "    angles = torch.cat([angles, angles], dim=1)  # Shape: (context_length, head_dim)\n",
    "\n",
    "    # Precompute sine and cosine\n",
    "    cos = torch.cos(angles)\n",
    "    sin = torch.sin(angles)\n",
    "\n",
    "    return cos, sin\n",
    "\n",
    "\n",
    "def apply_rope(x, cos, sin):\n",
    "    # x: (batch_size, num_heads, seq_len, head_dim)\n",
    "    batch_size, num_heads, seq_len, head_dim = x.shape\n",
    "    assert head_dim % 2 == 0, \"Head dimension must be even\"\n",
    "\n",
    "    # Split x into first half and second half\n",
    "    x1 = x[..., : head_dim // 2]  # First half\n",
    "    x2 = x[..., head_dim // 2 :]  # Second half\n",
    "\n",
    "    # Adjust sin and cos shapes\n",
    "    cos = cos[:seq_len, :].unsqueeze(0).unsqueeze(0)  # Shape: (1, 1, seq_len, head_dim)\n",
    "    sin = sin[:seq_len, :].unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    # Apply the rotary transformation\n",
    "    rotated = torch.cat((-x2, x1), dim=-1)\n",
    "    x_rotated = (x * cos) + (rotated * sin)\n",
    "\n",
    "    # It's ok to use lower-precision after applying cos and sin rotation\n",
    "    return x_rotated.to(dtype=x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8169ab5-f976-4222-a2e1-eb1cabf267cb",
   "metadata": {
    "id": "e8169ab5-f976-4222-a2e1-eb1cabf267cb"
   },
   "outputs": [],
   "source": [
    "# Grouped Query Attention mechanism with RoPE and optional normalization\n",
    "# this attention mechanism groups keys and values to reduce computation\n",
    "# it also applies rotary positional embeddings (RoPE) to the queries and keys to preserve positional information\n",
    "\n",
    "class GroupedQueryAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self, d_in, num_heads, num_kv_groups, head_dim=None, qk_norm=False, dtype=None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert num_heads % num_kv_groups == 0, \"num_heads must be divisible by num_kv_groups\"\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.num_kv_groups = num_kv_groups\n",
    "        self.group_size = num_heads // num_kv_groups\n",
    "\n",
    "        if head_dim is None:\n",
    "            assert d_in % num_heads == 0, \"`d_in` must be divisible by `num_heads` if `head_dim` is not set\"\n",
    "            head_dim = d_in // num_heads\n",
    "\n",
    "        self.head_dim = head_dim\n",
    "        self.d_out = num_heads * head_dim\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, self.d_out, bias=False, dtype=dtype)\n",
    "        self.W_key = nn.Linear(d_in, num_kv_groups * head_dim, bias=False, dtype=dtype)\n",
    "        self.W_value = nn.Linear(d_in, num_kv_groups * head_dim, bias=False, dtype=dtype)\n",
    "\n",
    "        self.out_proj = nn.Linear(self.d_out, d_in, bias=False, dtype=dtype)\n",
    "\n",
    "        if qk_norm:\n",
    "            self.q_norm = RMSNorm(head_dim, eps=1e-6)\n",
    "            self.k_norm = RMSNorm(head_dim, eps=1e-6)\n",
    "        else:\n",
    "            self.q_norm = self.k_norm = None\n",
    "\n",
    "    def forward(self, x, mask, cos, sin):\n",
    "        b, num_tokens, _ = x.shape\n",
    "\n",
    "        # Apply projections\n",
    "        queries = self.W_query(x)  # (b, num_tokens, num_heads * head_dim)\n",
    "        keys = self.W_key(x)       # (b, num_tokens, num_kv_groups * head_dim)\n",
    "        values = self.W_value(x)   # (b, num_tokens, num_kv_groups * head_dim)\n",
    "\n",
    "        # Reshape\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        keys = keys.view(b, num_tokens, self.num_kv_groups, self.head_dim).transpose(1, 2)\n",
    "        values = values.view(b, num_tokens, self.num_kv_groups, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Optional normalization\n",
    "        if self.q_norm:\n",
    "            queries = self.q_norm(queries)\n",
    "        if self.k_norm:\n",
    "            keys = self.k_norm(keys)\n",
    "\n",
    "        # Apply RoPE\n",
    "        queries = apply_rope(queries, cos, sin)\n",
    "        keys = apply_rope(keys, cos, sin)\n",
    "\n",
    "        # Expand K and V to match number of heads\n",
    "        keys = keys.repeat_interleave(self.group_size, dim=1)\n",
    "        values = values.repeat_interleave(self.group_size, dim=1)\n",
    "\n",
    "        # Attention\n",
    "        attn_scores = queries @ keys.transpose(2, 3)\n",
    "        attn_scores = attn_scores.masked_fill(mask, -torch.inf)\n",
    "        attn_weights = torch.softmax(attn_scores / self.head_dim**0.5, dim=-1)\n",
    "\n",
    "        context = (attn_weights @ values).transpose(1, 2).reshape(b, num_tokens, self.d_out)\n",
    "        return self.out_proj(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457cb2f8-50c1-4045-8a74-f181bfb5fea9",
   "metadata": {
    "id": "457cb2f8-50c1-4045-8a74-f181bfb5fea9"
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    # A single transformer block with grouped query attention and feed-forward network\n",
    "    # This block applies attention and feed-forward layers with residual connections and layer normalization.\n",
    "    # It is designed to process a sequence of input embeddings and produce an output sequence of the same length.\n",
    "    \n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = GroupedQueryAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            head_dim=cfg[\"head_dim\"],\n",
    "            num_kv_groups=cfg[\"n_kv_groups\"],\n",
    "            qk_norm=cfg[\"qk_norm\"],\n",
    "            dtype=cfg[\"dtype\"]\n",
    "        )\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = RMSNorm(cfg[\"emb_dim\"], eps=1e-6)\n",
    "        self.norm2 = RMSNorm(cfg[\"emb_dim\"], eps=1e-6)\n",
    "\n",
    "    def forward(self, x, mask, cos, sin):\n",
    "        # Shortcut connection for attention block\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x, mask, cos, sin)  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        # Shortcut connection for feed-forward block\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88de3e3-9f07-42cc-816b-28dbd46e96c4",
   "metadata": {
    "id": "e88de3e3-9f07-42cc-816b-28dbd46e96c4"
   },
   "outputs": [],
   "source": [
    "class Qwen3Model(nn.Module):\n",
    "    # setting up the Qwen3 model architecture\n",
    "    # This class defines the overall model structure, including token embeddings,\n",
    "    # multiple transformer blocks, final normalization, and output projection to vocabulary.\n",
    "    # It also includes mechanisms to capture embeddings for classification tasks.\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "        # Main model parameters\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"], dtype=cfg[\"dtype\"])\n",
    "\n",
    "        self.trf_blocks = nn.ModuleList(  # ModuleList since Sequential can only accept one input, and we need `x, mask, cos, sin`\n",
    "            [TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "\n",
    "        self.final_norm = RMSNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False, dtype=cfg[\"dtype\"])\n",
    "\n",
    "        # Reusuable utilities\n",
    "        if cfg[\"head_dim\"] is None:\n",
    "            head_dim = cfg[\"emb_dim\"] // cfg[\"n_heads\"]\n",
    "        else:\n",
    "            head_dim = cfg[\"head_dim\"]\n",
    "        cos, sin = compute_rope_params(\n",
    "            head_dim=head_dim,\n",
    "            theta_base=cfg[\"rope_base\"],\n",
    "            context_length=cfg[\"context_length\"]\n",
    "        )\n",
    "        self.register_buffer(\"cos\", cos, persistent=False)\n",
    "        self.register_buffer(\"sin\", sin, persistent=False)\n",
    "        self.cfg = cfg\n",
    "\n",
    "        # EMBEDDING HOOK FOR CLASSIFICATION\n",
    "        # These attributes allow us to capture the last hidden state (embeddings)\n",
    "        # before the final output projection. This is useful for:\n",
    "        # 1. Training a classifier on top of the language model\n",
    "        # 2. Extracting meaningful representations of text\n",
    "        # 3. Fine-tuning only the classifier while keeping the LM frozen\n",
    "\n",
    "        self.last_hidden_state = None  # Storage for the embeddings from final_norm\n",
    "        self._hook_enabled = False      # Flag to control when to capture embeddings\n",
    "\n",
    "    # HOOK CONTROL METHODS\n",
    "    def enable_embedding_hook(self):\n",
    "        \"\"\"\n",
    "        Enable capturing of embeddings from the last hidden layer.\n",
    "        \n",
    "        When enabled, the model will store the output of final_norm\n",
    "        (before the output head) in self.last_hidden_state during forward pass.\n",
    "        \n",
    "        Usage:\n",
    "            model.enable_embedding_hook()\n",
    "            output = model(input_ids)\n",
    "            embeddings = model.get_last_hidden_state()\n",
    "        \"\"\"\n",
    "        self._hook_enabled = True\n",
    "\n",
    "    def disable_embedding_hook(self):\n",
    "        \"\"\"\n",
    "        Disable embedding capture to save memory during normal text generation.\n",
    "        \n",
    "        Usage:\n",
    "            model.disable_embedding_hook()\n",
    "        \"\"\"\n",
    "        self._hook_enabled = False\n",
    "\n",
    "    def get_last_hidden_state(self):\n",
    "        \"\"\"\n",
    "        Retrieve the last captured hidden state (embeddings).\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Shape (batch_size, seq_len, emb_dim)\n",
    "                         The normalized embeddings before the output projection\n",
    "        \n",
    "        Note: Returns None if hook was not enabled during forward pass\n",
    "        \n",
    "        Usage:\n",
    "            embeddings = model.get_last_hidden_state()\n",
    "            # For classification, typically use embeddings[:, -1, :] (last token)\n",
    "            # or embeddings.mean(dim=1) (average pooling)\n",
    "        \"\"\"\n",
    "        return self.last_hidden_state\n",
    "\n",
    "\n",
    "    def forward(self, in_idx, return_embeddings=False):\n",
    "        \"\"\"\n",
    "        Forward pass through the model.\n",
    "        \n",
    "        Args:\n",
    "            in_idx: Input token indices (batch_size, seq_len)\n",
    "            return_embeddings: If True, return embeddings instead of logits\n",
    "        \n",
    "        Returns:\n",
    "            If return_embeddings=False: logits (batch_size, seq_len, vocab_size)\n",
    "            If return_embeddings=True: embeddings (batch_size, seq_len, emb_dim)\n",
    "        \"\"\"\n",
    "        # Forward pass\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        x = tok_embeds\n",
    "\n",
    "        num_tokens = x.shape[1]\n",
    "        mask = torch.triu(torch.ones(num_tokens, num_tokens, device=x.device, dtype=torch.bool), diagonal=1)\n",
    "        \n",
    "        for block in self.trf_blocks:\n",
    "            x = block(x, mask, self.cos, self.sin)\n",
    "        x = self.final_norm(x)\n",
    "        \n",
    "        # EMBEDDING CAPTURE \n",
    "        # Store embeddings if hook is enabled OR if explicitly requested\n",
    "        # This happens AFTER all transformer blocks and final normalization\n",
    "        # but BEFORE the output projection to vocabulary\n",
    "        if self._hook_enabled or return_embeddings:\n",
    "            self.last_hidden_state = x\n",
    "        \n",
    "        # If embeddings are requested, return them directly (for classification)\n",
    "        if return_embeddings:\n",
    "            return x\n",
    "        # Otherwise, return logits for language modeling\n",
    "        \n",
    "        logits = self.out_head(x.to(self.cfg[\"dtype\"]))\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b345491-3510-4397-92d3-cd0a3fa3deee",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "# 2. Create Tokenizer   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dd5425",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'pip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m get_ipython().system(\u001b[33m'\u001b[39m\u001b[33mpip install datasets sentencepiece\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Load TinyStories dataset\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLoading TinyStories dataset...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m ds = load_dataset(\u001b[33m\"\u001b[39m\u001b[33mroneneldan/TinyStories\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install datasets sentencepiece\n",
    "\n",
    "# Load TinyStories dataset\n",
    "from datasets import load_dataset\n",
    "print(\"Loading TinyStories dataset...\")\n",
    "ds = load_dataset(\"roneneldan/TinyStories\")\n",
    "print(f\"Dataset loaded! Train split has {len(ds['train'])} stories\")\n",
    "\n",
    "# Export a subset to text file for tokenizer training\n",
    "NUM_STORIES = 100000\n",
    "print(f\"\\nExporting {NUM_STORIES:,} stories to tiny_stories.txt...\")\n",
    "with open(\"tiny_stories.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for story in ds[\"train\"][\"text\"][:NUM_STORIES]:\n",
    "        f.write(story.strip() + \"\\n\")\n",
    "print(f\"Export complete! {NUM_STORIES:,} stories exported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b851561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SentencePiece tokenizer\n",
    "# choosing sentencepiece as it is a popular and efficient tokenizer library used in many NLP models including Google's T5 and ALBERT and it should be good for a qwen3 small model with the tiny stories dataset\n",
    "import sentencepiece as spm\n",
    "\n",
    "# tiny stories data set shouldn't have a huge vocabulary, so we are setting it to 8000 to limit the parameters on Qwen3 model\n",
    "print(\"Training SentencePiece tokenizer...\")\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input=\"tiny_stories.txt\",\n",
    "    model_prefix=\"qwen3_tokenizer\",\n",
    "    vocab_size=8000,\n",
    "    model_type=\"unigram\",\n",
    "    pad_id=0,\n",
    "    unk_id=1,\n",
    "    bos_id=2,\n",
    "    eos_id=3,\n",
    ")\n",
    "print(\"Tokenizer training complete!\")\n",
    "\n",
    "# Create wrapper class for easy integration with model\n",
    "class SimpleTokenizer:\n",
    "    \"\"\"Wrapper around SentencePiece for Qwen3 model compatibility\"\"\"\n",
    "    def __init__(self, model_path):\n",
    "        self.sp = spm.SentencePieceProcessor(model_file=model_path)\n",
    "        self.vocab_size = self.sp.vocab_size()\n",
    "        self.eos_token_id = self.sp.eos_id()\n",
    "        self.pad_token_id = self.sp.pad_id()\n",
    "        self.bos_token_id = self.sp.bos_id()\n",
    "        self.unk_token_id = self.sp.unk_id()\n",
    "    \n",
    "    def encode(self, text):\n",
    "        \"\"\"Encode text to token IDs\"\"\"\n",
    "        return self.sp.encode(text, out_type=int)\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        \"\"\"Decode token IDs to text\"\"\"\n",
    "        return self.sp.decode(ids)\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = SimpleTokenizer(\"qwen3_tokenizer.model\")\n",
    "print(f\"\\nTokenizer initialized!\")\n",
    "print(f\"Vocab size: {tokenizer.vocab_size}\")\n",
    "print(f\"PAD token ID: {tokenizer.pad_token_id}\")\n",
    "print(f\"EOS token ID: {tokenizer.eos_token_id}\")\n",
    "\n",
    "# Test the tokenizer\n",
    "test_text = \"Once upon a time\"\n",
    "test_ids = tokenizer.encode(test_text)\n",
    "print(f\"\\nTest encoding:\")\n",
    "print(f\"Text: '{test_text}'\")\n",
    "print(f\"Token IDs: {test_ids}\")\n",
    "print(f\"Decoded: '{tokenizer.decode(test_ids)}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2d201f-74ad-4d63-ab9c-601b00674a48",
   "metadata": {
    "id": "be2d201f-74ad-4d63-ab9c-601b00674a48"
   },
   "source": [
    "&nbsp;\n",
    "# 3. Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa142fa-b375-4e78-b392-2072ced666f3",
   "metadata": {
    "id": "caa142fa-b375-4e78-b392-2072ced666f3"
   },
   "outputs": [],
   "source": [
    "# set to the Small model, we are also leaving in the original code from the reference config for the 0.6B model\n",
    "# for backup purposes, all others in the template were removed.\n",
    "CHOOSE_MODEL = \"Small\"\n",
    "\n",
    "if CHOOSE_MODEL == \"0.6B\":\n",
    "    # Original reference config (kept as backup)\n",
    "    QWEN3_CONFIG = {\n",
    "        \"vocab_size\": 151_936,\n",
    "        \"context_length\": 40_960,\n",
    "        \"emb_dim\": 1024,\n",
    "        \"n_heads\": 16,\n",
    "        \"n_layers\": 28,\n",
    "        \"hidden_dim\": 3072,\n",
    "        \"head_dim\": 128,\n",
    "        \"qk_norm\": True,\n",
    "        \"n_kv_groups\": 8,\n",
    "        \"rope_base\": 1_000_000.0,\n",
    "        \"dtype\": torch.bfloat16,\n",
    "    }\n",
    "\n",
    "elif CHOOSE_MODEL == \"Small\":\n",
    "    # Small model targeting ~180-200M parameters\n",
    "    # IMPORTANT: vocab_size must match tokenizer.vocab_size from Section 2\n",
    "    QWEN3_CONFIG = {\n",
    "        \"vocab_size\": tokenizer.vocab_size,  # Updated from tokenizer\n",
    "        \"context_length\": 1024,              # Context window for training\n",
    "        \"emb_dim\": 896,                      # Embedding dimension = n_heads * head_dim\n",
    "        \"n_heads\": 14,                       # Number of attention heads\n",
    "        \"n_layers\": 16,                      # Number of transformer layers (more layers = better reasoning)\n",
    "        \"hidden_dim\": 3584,                  # FFN hidden dim (4x emb_dim)\n",
    "        \"head_dim\": 64,                      # Size of each attention head\n",
    "        \"qk_norm\": True,                     # Normalize queries and keys in GQA\n",
    "        \"n_kv_groups\": 7,                    # Key-Value groups (n_heads/2)\n",
    "        \"rope_base\": 10_000.0,               # RoPE theta base\n",
    "        \"dtype\": torch.bfloat16,             # Lower precision for memory efficiency\n",
    "    }\n",
    "    \n",
    "else:\n",
    "    raise ValueError(f\"{CHOOSE_MODEL} is not supported.\")\n",
    "\n",
    "print(f\"Config for {CHOOSE_MODEL} model:\")\n",
    "print(f\"  Vocab size: {QWEN3_CONFIG['vocab_size']:,}\")\n",
    "print(f\"  Embedding dim: {QWEN3_CONFIG['emb_dim']}\")\n",
    "print(f\"  Layers: {QWEN3_CONFIG['n_layers']}\")\n",
    "print(f\"  Heads: {QWEN3_CONFIG['n_heads']}\")\n",
    "print(f\"  Context length: {QWEN3_CONFIG['context_length']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156253fe-aacd-4da2-8f13-705f05c4b11e",
   "metadata": {
    "id": "156253fe-aacd-4da2-8f13-705f05c4b11e"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(528)\n",
    "model = Qwen3Model(QWEN3_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf86265-4e9d-4024-9ed0-99076944e304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen3Model(\n",
       "  (tok_emb): Embedding(151936, 1024)\n",
       "  (trf_blocks): ModuleList(\n",
       "    (0-27): 28 x TransformerBlock(\n",
       "      (att): GroupedQueryAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=2048, bias=False)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (out_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "        (q_norm): RMSNorm()\n",
       "        (k_norm): RMSNorm()\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "        (fc2): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "        (fc3): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "      )\n",
       "      (norm1): RMSNorm()\n",
       "      (norm2): RMSNorm()\n",
       "    )\n",
       "  )\n",
       "  (final_norm): RMSNorm()\n",
       "  (out_head): Linear(in_features=1024, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90aca91d-4bee-45ce-993a-4ec5393abe2b",
   "metadata": {},
   "source": [
    "- A quick check that the forward pass works before continuing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf0a6b7-b688-42c9-966e-c223d34db99f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2256, -0.0164, -0.7070,  ...,  0.4414,  0.1245,  1.0703],\n",
       "         [-0.6602,  0.5352, -0.0718,  ..., -0.0737,  0.5391,  0.3086],\n",
       "         [-0.4785, -0.1562,  0.1045,  ..., -0.2324,  0.2354,  0.6328]]],\n",
       "       dtype=torch.bfloat16, grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.tensor([1, 2, 3]).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364e76ca-52f8-4fa5-af37-c4069f9694bc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "364e76ca-52f8-4fa5-af37-c4069f9694bc",
    "outputId": "00d7e983-262e-4c65-f322-f4d999311988"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 751,632,384\n",
      "\n",
      "Total number of unique parameters: 596,049,920\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")\n",
    "\n",
    "# Account for weight tying\n",
    "total_params_normalized = total_params - model.tok_emb.weight.numel()\n",
    "print(f\"\\nTotal number of unique parameters: {total_params_normalized:,}\")\n",
    "\n",
    "#model has approximately the 200M parameter target, slightly over, but performance still runs in about 30 minutes per epoch on 1 A100 40GB GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5efb03-5a07-46e8-8607-93ed47549d2b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fd5efb03-5a07-46e8-8607-93ed47549d2b",
    "outputId": "65c1a95e-b502-4150-9e2e-da619d9053d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32 (PyTorch default): 5.64 GB\n",
      "bfloat16: 2.82 GB\n"
     ]
    }
   ],
   "source": [
    "def model_memory_size(model, input_dtype=torch.float32):\n",
    "    total_params = 0\n",
    "    total_grads = 0\n",
    "    for param in model.parameters():\n",
    "        # Calculate total number of elements per parameter\n",
    "        param_size = param.numel()\n",
    "        total_params += param_size\n",
    "        # Check if gradients are stored for this parameter\n",
    "        if param.requires_grad:\n",
    "            total_grads += param_size\n",
    "\n",
    "    # Calculate buffer size (non-parameters that require memory)\n",
    "    total_buffers = sum(buf.numel() for buf in model.buffers())\n",
    "\n",
    "    # Size in bytes = (Number of elements) * (Size of each element in bytes)\n",
    "    # We assume parameters and gradients are stored in the same type as input dtype\n",
    "    element_size = torch.tensor(0, dtype=input_dtype).element_size()\n",
    "    total_memory_bytes = (total_params + total_grads + total_buffers) * element_size\n",
    "\n",
    "    # Convert bytes to gigabytes\n",
    "    total_memory_gb = total_memory_bytes / (1024**3)\n",
    "\n",
    "    return total_memory_gb\n",
    "\n",
    "print(f\"float32 (PyTorch default): {model_memory_size(model, input_dtype=torch.float32):.2f} GB\")\n",
    "print(f\"bfloat16: {model_memory_size(model, input_dtype=torch.bfloat16):.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f12baf-f79b-499f-85c0-51328a6a20f5",
   "metadata": {
    "id": "31f12baf-f79b-499f-85c0-51328a6a20f5"
   },
   "outputs": [],
   "source": [
    "# set device to GPU if available, without this the CPU is way too slow...\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aclucp6lyg9",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "# 4. Train Model on TinyStories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5gz38sox7iy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data with 60/20/20 train/val/test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"Preparing training data with 60/20/20 split...\")\n",
    "all_stories = ds[\"train\"][\"text\"][:100000]\n",
    "\n",
    "# First split: 60% train, 40% which will become 20% val + 20% test\n",
    "train_stories, temp_stories = train_test_split(\n",
    "    all_stories,\n",
    "    test_size=0.4,  # 40% for val+test\n",
    "    random_state=528\n",
    ")\n",
    "\n",
    "# Second split: split the 40% into 20% val and 20% test\n",
    "val_stories, test_stories = train_test_split(\n",
    "    temp_stories,\n",
    "    test_size=0.5,  # 50% of 40% = 20% of total\n",
    "    random_state=528\n",
    ")\n",
    "\n",
    "print(f\"Train stories: {len(train_stories):,} (60%)\")      # 60,000\n",
    "print(f\"Validation stories: {len(val_stories):,} (20%)\")   # 20,000\n",
    "print(f\"Test stories: {len(test_stories):,} (20%)\")        # 20,000\n",
    "print(f\"Total: {len(train_stories) + len(val_stories) + len(test_stories):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aq7qrub6os",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PyTorch datasets\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TinyStoriesDataset(Dataset):\n",
    "    def __init__(self, stories, tokenizer, max_length=256):\n",
    "        self.stories = stories\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.stories)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        story = self.stories[idx]\n",
    "        # Tokenize\n",
    "        tokens = self.tokenizer.encode(story)\n",
    "        \n",
    "        # Truncate or pad to max_length\n",
    "        if len(tokens) > self.max_length:\n",
    "            tokens = tokens[:self.max_length]\n",
    "        else:\n",
    "            # Pad with pad_token_id\n",
    "            tokens = tokens + [self.tokenizer.pad_token_id] * (self.max_length - len(tokens))\n",
    "        \n",
    "        # Convert to tensor\n",
    "        tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "        \n",
    "        # For language modeling, input is tokens[:-1], target is tokens[1:]\n",
    "        return tokens[:-1], tokens[1:]\n",
    "\n",
    "# Create datasets and set to a consistent max_length to allow the nn to run.\n",
    "# when the story has too many tokens, it will be truncated, when too few, it will be padded.\n",
    "\n",
    "train_dataset = TinyStoriesDataset(train_stories, tokenizer, max_length=256)\n",
    "val_dataset = TinyStoriesDataset(val_stories, tokenizer, max_length=256)\n",
    "test_dataset = TinyStoriesDataset(test_stories, tokenizer, max_length=256)\n",
    "\n",
    "print(f\"Train dataset: {len(train_dataset)} examples\")\n",
    "print(f\"Val dataset: {len(val_dataset)} examples\")\n",
    "print(f\"Test dataset: {len(test_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vueopvba2cr",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "BATCH_SIZE = 16  # Adjust based on GPU memory\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"Batches per epoch: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fn5pkqx4nc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup with early stopping for computational efficiency, also allows us to crank up the epochs for a little more powerful model, but stop before overfitting\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# Hyperparameters\n",
    "LEARNING_RATE = 3e-4\n",
    "NUM_EPOCHS = 10  # will stop early if needed\n",
    "EVAL_INTERVAL = 500  # Validate every 500 steps\n",
    "PATIENCE = 5  # Stop if no improvement for 5 validation checks\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.1)\n",
    "\n",
    "# Loss function (CrossEntropyLoss for language modeling)\n",
    "def calculate_loss(logits, targets):\n",
    "    # Reshape for cross entropy: (batch * seq_len, vocab_size) and (batch * seq_len)\n",
    "    logits_flat = logits.view(-1, logits.size(-1))\n",
    "    targets_flat = targets.view(-1)\n",
    "    return F.cross_entropy(logits_flat, targets_flat, ignore_index=tokenizer.pad_token_id)\n",
    "\n",
    "# Validation function\n",
    "def validate(model, val_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            logits = model(inputs)\n",
    "            loss = calculate_loss(logits, targets)\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "    model.train()\n",
    "    return avg_loss\n",
    "\n",
    "# Early stopping tracker\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0.001):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.should_stop = False\n",
    "    \n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            print(f\"    EarlyStopping counter: {self.counter}/{self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.should_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "\n",
    "early_stopping = EarlyStopping(patience=PATIENCE)\n",
    "\n",
    "print(\"Training setup complete!\")\n",
    "print(f\"Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"Max epochs: {NUM_EPOCHS}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Eval interval: {EVAL_INTERVAL} steps\")\n",
    "print(f\"Early stopping patience: {PATIENCE} checks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "x9yw29tzg5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training loop with early stopping\n",
    "model.train()\n",
    "global_step = 0\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_model_state = None\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "print(\"Starting training...\\n\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Epoch {epoch + 1}/{NUM_EPOCHS}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        # Move to device\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(inputs)\n",
    "        loss = calculate_loss(logits, targets)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track loss\n",
    "        train_losses.append(loss.item())\n",
    "        epoch_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        global_step += 1\n",
    "        \n",
    "        # Print progress every 100 steps\n",
    "        if (batch_idx + 1) % 100 == 0:\n",
    "            avg_loss = epoch_loss / num_batches\n",
    "            print(f\"  Step {global_step:5d} | Batch {batch_idx + 1:4d}/{len(train_loader)} | Train Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        # Validation every EVAL_INTERVAL steps\n",
    "        if global_step % EVAL_INTERVAL == 0:\n",
    "            val_loss = validate(model, val_loader, device)\n",
    "            val_losses.append((global_step, val_loss))\n",
    "            print(f\"  → Validation at step {global_step}: Val Loss = {val_loss:.4f}\")\n",
    "            \n",
    "            # Save best model\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_model_state = model.state_dict().copy()\n",
    "                print(f\"    ✓ New best model! (Val Loss: {val_loss:.4f})\")\n",
    "            \n",
    "            # Check early stopping\n",
    "            early_stopping(val_loss)\n",
    "            if early_stopping.should_stop:\n",
    "                print(f\"\\n  Early stopping triggered at step {global_step}\")\n",
    "                print(f\"  Best validation loss: {best_val_loss:.4f}\")\n",
    "                break\n",
    "    \n",
    "    # Break outer loop if early stopping triggered\n",
    "    if early_stopping.should_stop:\n",
    "        break\n",
    "    \n",
    "    # End of epoch validation\n",
    "    epoch_avg_loss = epoch_loss / num_batches\n",
    "    val_loss = validate(model, val_loader, device)\n",
    "    val_losses.append((global_step, val_loss))\n",
    "    \n",
    "    print(f\"\\n  Epoch {epoch + 1} Complete:\")\n",
    "    print(f\"    Average Train Loss: {epoch_avg_loss:.4f}\")\n",
    "    print(f\"    Validation Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_state = model.state_dict().copy()\n",
    "        print(f\"    ✓ New best model! (Val Loss: {val_loss:.4f})\")\n",
    "    \n",
    "    # Check early stopping at end of epoch\n",
    "    early_stopping(val_loss)\n",
    "    if early_stopping.should_stop:\n",
    "        print(f\"\\n  Early stopping triggered after epoch {epoch + 1}\")\n",
    "        print(f\"  Best validation loss: {best_val_loss:.4f}\")\n",
    "        break\n",
    "    \n",
    "    print()  # Blank line between epochs for readability\n",
    "\n",
    "# Load best model\n",
    "if best_model_state is not None:\n",
    "    model.load_state_dict(best_model_state)\n",
    "    print(f\"\\n✓ Loaded best model with validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qno3asre2ln",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training progress\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Training loss over steps\n",
    "ax1.plot(train_losses, alpha=0.6, label='Train Loss (per batch)')\n",
    "# Add smoothed line\n",
    "window = 100\n",
    "if len(train_losses) > window:\n",
    "    smoothed = [sum(train_losses[max(0, i-window):i+1]) / min(i+1, window) \n",
    "                for i in range(len(train_losses))]\n",
    "    ax1.plot(smoothed, linewidth=2, label='Smoothed Train Loss')\n",
    "ax1.set_xlabel('Training Step')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training Loss (Step-by-Step)')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Validation loss over time\n",
    "val_steps = [step for step, _ in val_losses]\n",
    "val_loss_values = [loss for _, loss in val_losses]\n",
    "ax2.plot(val_steps, val_loss_values, 'o-', linewidth=2, markersize=8, label='Validation Loss')\n",
    "ax2.set_xlabel('Training Step')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.set_title('Validation Loss (At Validation Steps)')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nValidation checkpoints:\")\n",
    "for step, loss in val_losses:\n",
    "    print(f\"  Step {step:5d}: Val Loss = {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4xnuyu4unta",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "# 5. Save Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fyxlpwlcx1r",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'config': QWEN3_CONFIG,\n",
    "    'train_losses': train_losses,\n",
    "    'val_losses': val_losses,\n",
    "    'best_val_loss': best_val_loss,\n",
    "    'tokenizer_path': 'qwen3_tokenizer.model'\n",
    "}, 'qwen3_small_trained.pth')\n",
    "\n",
    "print(\"✓ Model saved to 'qwen3_small_trained.pth'\")\n",
    "print(f\"  Best validation loss: {best_val_loss:.4f}\")\n",
    "print(f\"  Total training steps: {global_step}\")\n",
    "print(f\"  Config: {QWEN3_CONFIG['n_layers']} layers, {QWEN3_CONFIG['emb_dim']} emb_dim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03556d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate on test set\n",
    "test_loss = validate(model, test_loader, device)\n",
    "print(f\"\\nTest Loss on Hold Out Stories: {test_loss:.4f}\")\n",
    "\n",
    "# Plot 3: Test loss visualization\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.bar(['Test Set'], [test_loss], color='skyblue')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Test Loss on Hold Out Stories')\n",
    "plt.ylim(0, max(1.0, test_loss + 0.5))\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dw5yen9x61q",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "# 6. Test Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "n12lzhuwdh",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text generation function with temperature sampling, we will set the device to CPU as a fallback here in case anything happens, but we later set it back to the gpu if available when the function is called\n",
    "\n",
    "def generate_text(model, tokenizer, prompt, max_tokens=30, temperature=1.0, device='cpu'):\n",
    "    \"\"\"Generate text from the trained model\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Encode prompt\n",
    "    token_ids = tokenizer.encode(prompt)\n",
    "    token_ids = torch.tensor(token_ids, device=device).unsqueeze(0)\n",
    "    \n",
    "    generated_tokens = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_tokens):\n",
    "            # Get logits\n",
    "            logits = model(token_ids)[:, -1, :]  # Last token logits\n",
    "            \n",
    "            # Apply temperature\n",
    "            logits = logits / temperature\n",
    "            \n",
    "            # Sample from distribution\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            # Check for EOS\n",
    "            if next_token.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "            \n",
    "            generated_tokens.append(next_token.item())\n",
    "            token_ids = torch.cat([token_ids, next_token], dim=1)\n",
    "    \n",
    "    # Decode\n",
    "    full_text = tokenizer.decode(tokenizer.encode(prompt) + generated_tokens)\n",
    "    return full_text\n",
    "\n",
    "print(\"Text generation function ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "x6v15mx8h6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test generation with different temperatures and prompts\n",
    "# testing with the classic \"Once upon a time\", a couple from media, and a random prompt to see how the model performs\n",
    "test_prompts = [\"Once upon a time\", \"In a galaxy far far away\", \"On Thursday the princess ate a cookie\", \"Who lives in a pineapple under the sea?\"]\n",
    "temperatures = [0.2, 0.5, 0.8, 1.0]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f\"📝 Testing prompt: '{prompt}'\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    for temp in temperatures:\n",
    "        print(f\"\\nTemperature {temp}:\")\n",
    "        generated = generate_text(model, tokenizer, prompt, max_tokens=30, temperature=temp, device=device)\n",
    "        print(f\"  {generated}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "otq0nvizrki",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "# 7. Sentiment Classifier Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bct2gltr9s",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load emotions dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load classification data from GitHub\n",
    "emotions_df = pd.read_csv('https://raw.githubusercontent.com/tskunz/GenAI_Midterm_Qwen3_Small/refs/heads/main/emotions_classified.csv')\n",
    "\n",
    "print(f\"Emotions dataset loaded: {len(emotions_df)} examples\")\n",
    "print(f\"\\nColumns: {list(emotions_df.columns)}\")\n",
    "print(f\"\\nEmotion distribution:\")\n",
    "print(emotions_df['emotion'].value_counts())\n",
    "print(f\"\\nSentiment distribution:\")\n",
    "print(emotions_df['sentiment'].value_counts())\n",
    "\n",
    "# Show sample\n",
    "print(f\"\\nSample rows:\")\n",
    "print(emotions_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2210vo2iu",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare sentiment classification data\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "NUM_CLASSIFICATION_SAMPLES = min(5000, len(emotions_df))  # Use up to 5000\n",
    "\n",
    "# Sample the data\n",
    "emotions_sample = emotions_df.sample(n=NUM_CLASSIFICATION_SAMPLES, random_state=528)\n",
    "\n",
    "# Encode sentiment labels (positive/negative -> 0/1)\n",
    "label_encoder = LabelEncoder()\n",
    "emotions_sample['sentiment_encoded'] = label_encoder.fit_transform(emotions_sample['sentiment'])\n",
    "\n",
    "print(f\"Using {NUM_CLASSIFICATION_SAMPLES} samples for classification\")\n",
    "print(f\"Label mapping: {dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))}\")\n",
    "\n",
    "# Split into train/test (80/20)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(\n",
    "    emotions_sample, \n",
    "    test_size=0.2, \n",
    "    random_state=528,\n",
    "    stratify=emotions_sample['sentiment_encoded']\n",
    ")\n",
    "\n",
    "print(f\"\\nClassification split:\")\n",
    "print(f\"  Train: {len(train_df)} examples\")\n",
    "print(f\"  Test: {len(test_df)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "g16tk9it5wi",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract embeddings from trained language model\n",
    "def extract_embeddings(texts, model, tokenizer, device, max_length=128):\n",
    "    \"\"\"Extract embeddings from the last hidden layer using the embedding hook\"\"\"\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for text in texts:\n",
    "            # Tokenize\n",
    "            tokens = tokenizer.encode(text)\n",
    "            if len(tokens) > max_length:\n",
    "                tokens = tokens[:max_length]\n",
    "            \n",
    "            # Pad if needed\n",
    "            if len(tokens) < max_length:\n",
    "                tokens = tokens + [tokenizer.pad_token_id] * (max_length - len(tokens))\n",
    "            \n",
    "            # Convert to tensor\n",
    "            input_ids = torch.tensor([tokens], device=device)\n",
    "            \n",
    "            # Get embeddings using return_embeddings parameter\n",
    "            hidden_states = model(input_ids, return_embeddings=True)\n",
    "            \n",
    "            # Use mean pooling over sequence dimension (ignoring padding)\n",
    "            # Shape: (1, seq_len, emb_dim) -> (emb_dim,)\n",
    "            mask = (input_ids != tokenizer.pad_token_id).float().unsqueeze(-1)\n",
    "            masked_hidden = hidden_states * mask\n",
    "            summed = masked_hidden.sum(dim=1)\n",
    "            counts = mask.sum(dim=1)\n",
    "            embedding = (summed / counts).squeeze(0)  # (emb_dim,)\n",
    "            \n",
    "            embeddings.append(embedding.cpu())\n",
    "    \n",
    "    return torch.stack(embeddings)\n",
    "\n",
    "# Extract embeddings for train and test\n",
    "print(\"Extracting embeddings from trained model...\")\n",
    "print(\"  This may take a few minutes...\")\n",
    "\n",
    "train_embeddings = extract_embeddings(train_df['text'].tolist(), model, tokenizer, device)\n",
    "test_embeddings = extract_embeddings(test_df['text'].tolist(), model, tokenizer, device)\n",
    "\n",
    "print(f\"✓ Train embeddings shape: {train_embeddings.shape}\")\n",
    "print(f\"✓ Test embeddings shape: {test_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p3c6vhj2ori",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sentiment classifier (simple feedforward on top of frozen embeddings)\n",
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, embeddings):\n",
    "        return self.classifier(embeddings)\n",
    "\n",
    "# Initialize classifier\n",
    "classifier = SentimentClassifier(embedding_dim=QWEN3_CONFIG['emb_dim'], num_classes=2).to(device)\n",
    "\n",
    "print(f\"Sentiment classifier initialized\")\n",
    "print(f\"  Input dim: {QWEN3_CONFIG['emb_dim']}\")\n",
    "print(f\"  Output classes: 2 (positive/negative)\")\n",
    "print(f\"  Total classifier parameters: {sum(p.numel() for p in classifier.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anfzaabemxb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train sentiment classifier\n",
    "classifier_optimizer = torch.optim.Adam(classifier.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Prepare data\n",
    "train_labels = torch.tensor(train_df['sentiment_encoded'].values, dtype=torch.long).to(device)\n",
    "test_labels = torch.tensor(test_df['sentiment_encoded'].values, dtype=torch.long).to(device)\n",
    "train_embeddings = train_embeddings.to(device)\n",
    "test_embeddings = test_embeddings.to(device)\n",
    "\n",
    "# Training loop for classifier\n",
    "NUM_CLASSIFIER_EPOCHS = 20\n",
    "BATCH_SIZE_CLASSIFIER = 64\n",
    "\n",
    "print(\"Training sentiment classifier...\")\n",
    "print(f\"  Epochs: {NUM_CLASSIFIER_EPOCHS}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE_CLASSIFIER}\")\n",
    "\n",
    "classifier.train()\n",
    "for epoch in range(NUM_CLASSIFIER_EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    # Mini-batch training\n",
    "    for i in range(0, len(train_embeddings), BATCH_SIZE_CLASSIFIER):\n",
    "        batch_embeddings = train_embeddings[i:i+BATCH_SIZE_CLASSIFIER]\n",
    "        batch_labels = train_labels[i:i+BATCH_SIZE_CLASSIFIER]\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = classifier(batch_embeddings)\n",
    "        loss = criterion(outputs, batch_labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        classifier_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        classifier_optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        num_batches += 1\n",
    "    \n",
    "    # Print progress\n",
    "    avg_loss = epoch_loss / num_batches\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"  Epoch {epoch+1}/{NUM_CLASSIFIER_EPOCHS} | Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"\\n✓ Classifier training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dcb383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate classifier (Assignment Requirements v.1, v.2, v.3)\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "classifier.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = classifier(test_embeddings)\n",
    "    test_predictions = torch.argmax(test_outputs, dim=1).cpu().numpy()\n",
    "\n",
    "test_labels_np = test_labels.cpu().numpy()\n",
    "\n",
    "# 1. Accuracy (must be > 50%)\n",
    "accuracy = accuracy_score(test_labels_np, test_predictions)\n",
    "\n",
    "# 2. Classification Report (includes precision, recall, f1)\n",
    "print(\"=\"*60)\n",
    "print(\"SENTIMENT CLASSIFIER EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n1. Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "if accuracy > 0.5:\n",
    "    print(\"   ✓ Requirement met: Accuracy > 50%\")\n",
    "else:\n",
    "    print(\"   ✗ Warning: Accuracy below 50%\")\n",
    "\n",
    "print(f\"\\n2. Classification Report (Precision, Recall, F1):\")\n",
    "print(classification_report(test_labels_np, test_predictions, target_names=label_encoder.classes_))\n",
    "\n",
    "# 3. Confusion Matrix\n",
    "cm = confusion_matrix(test_labels_np, test_predictions)\n",
    "print(f\"3. Confusion Matrix:\")\n",
    "print(f\"   True Negatives:  {cm[0,0]:4d} | False Positives: {cm[0,1]:4d}\")\n",
    "print(f\"   False Negatives: {cm[1,0]:4d} | True Positives:  {cm[1,1]:4d}\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_encoder.classes_)\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title(\"Confusion Matrix - Sentiment Classification\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
